{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn 과제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(max_depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:,'V1':'V28']\n",
    "Y = data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "\n",
       "         V8        V9       V10    ...          V19       V20       V21  \\\n",
       "0  0.098698  0.363787  0.090794    ...     0.403993  0.251412 -0.018307   \n",
       "1  0.085102 -0.255425 -0.166974    ...    -0.145783 -0.069083 -0.225775   \n",
       "\n",
       "        V22       V23       V24       V25       V26       V27       V28  \n",
       "0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  \n",
       "1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724  \n",
       "\n",
       "[2 rows x 28 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest를 이용한 변수 Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify = clf.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0. , 0.1, 0.1, 0. , 0. , 0. , 0. , 0. , 0.2, 0.1, 0. , 0. ,\n",
       "       0.1, 0. , 0. , 0.4, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
       "       0. , 0. ])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev =[]\n",
    "for i in range(len(a)):\n",
    "    if a[i] != 0:\n",
    "        ev.append('V'+str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['V3', 'V4', 'V10', 'V11', 'V14', 'V17']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습데이터 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[ev],Y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 분류알고리즘 실행(DicisionTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf_t = clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_t=clf.predict(X_test)\n",
    "pred_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모형검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Class</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56850</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Class      0   1\n",
       "row_0           \n",
       "0      56850  22\n",
       "1         23  67"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(pred_t,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Type 0</th>\n",
       "      <th>Predicted  Type 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Type 0</th>\n",
       "      <td>56850</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Type 1</th>\n",
       "      <td>22</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Predicted Type 0  Predicted  Type 1\n",
       "True Type 0             56850                 23\n",
       "True Type 1                22                 67"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matrix = pd.DataFrame(\n",
    "    confusion_matrix(y_test, pred_t),\n",
    "    columns=['Predicted Type 0', 'Predicted  Type 1'],\n",
    "    index=['True Type 0', 'True Type 1']\n",
    ")\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy:  0.9992099996488887\n"
     ]
    }
   ],
   "source": [
    "print('Prediction Accuracy: ', clf.score(X_test,  y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree그림그리기~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "            Time         V1         V2        V3        V4        V5  \\\n0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n5            2.0  -0.425966   0.960523  1.141109 -0.168252  0.420987   \n6            4.0   1.229658   0.141004  0.045371  1.202613  0.191881   \n7            7.0  -0.644269   1.417964  1.074380 -0.492199  0.948934   \n8            7.0  -0.894286   0.286157 -0.113192 -0.271526  2.669599   \n9            9.0  -0.338262   1.119593  1.044367 -0.222187  0.499361   \n10          10.0   1.449044  -1.176339  0.913860 -1.375667 -1.971383   \n11          10.0   0.384978   0.616109 -0.874300 -0.094019  2.924584   \n12          10.0   1.249999  -1.221637  0.383930 -1.234899 -1.485419   \n13          11.0   1.069374   0.287722  0.828613  2.712520 -0.178398   \n14          12.0  -2.791855  -0.327771  1.641750  1.767473 -0.136588   \n15          12.0  -0.752417   0.345485  2.057323 -1.468643 -1.158394   \n16          12.0   1.103215  -0.040296  1.267332  1.289091 -0.735997   \n17          13.0  -0.436905   0.918966  0.924591 -0.727219  0.915679   \n18          14.0  -5.401258  -5.450148  1.186305  1.736239  3.049106   \n19          15.0   1.492936  -1.029346  0.454795 -1.438026 -1.555434   \n20          16.0   0.694885  -1.361819  1.029221  0.834159 -1.191209   \n21          17.0   0.962496   0.328461 -0.171479  2.109204  1.129566   \n22          18.0   1.166616   0.502120 -0.067300  2.261569  0.428804   \n23          18.0   0.247491   0.277666  1.185471 -0.092603 -1.314394   \n24          22.0  -1.946525  -0.044901 -0.405570 -1.013057  2.941968   \n25          22.0  -2.074295  -0.121482  1.322021  0.410008  0.295198   \n26          23.0   1.173285   0.353498  0.283905  1.133563 -0.172577   \n27          23.0   1.322707  -0.174041  0.434555  0.576038 -0.836758   \n28          23.0  -0.414289   0.905437  1.727453  1.473471  0.007443   \n29          23.0   1.059387  -0.175319  1.266130  1.186110 -0.786002   \n...          ...        ...        ...       ...       ...       ...   \n284777  172764.0   2.079137  -0.028723 -1.343392  0.358000 -0.045791   \n284778  172764.0  -0.764523   0.588379 -0.907599 -0.418847  0.901528   \n284779  172766.0   1.975178  -0.616244 -2.628295 -0.406246  2.327804   \n284780  172766.0  -1.727503   1.108356  2.219561  1.148583 -0.884199   \n284781  172766.0  -1.139015  -0.155510  1.894478 -1.138957  1.451777   \n284782  172767.0  -0.268061   2.540315 -1.400915  4.846661  0.639105   \n284783  172768.0  -1.796092   1.929178 -2.828417 -1.689844  2.199572   \n284784  172768.0  -0.669662   0.923769 -1.543167 -1.560729  2.833960   \n284785  172768.0   0.032887   0.545338 -1.185844 -1.729828  2.932315   \n284786  172768.0  -2.076175   2.142238 -2.522704 -1.888063  1.982785   \n284787  172769.0  -1.029719  -1.110670 -0.636179 -0.840816  2.424360   \n284788  172770.0   2.007418  -0.280235 -0.208113  0.335261 -0.715798   \n284789  172770.0  -0.446951   1.302212 -0.168583  0.981577  0.578957   \n284790  172771.0  -0.515513   0.971950 -1.014580 -0.677037  0.912430   \n284791  172774.0  -0.863506   0.874701  0.420358 -0.530365  0.356561   \n284792  172774.0  -0.724123   1.485216 -1.132218 -0.607190  0.709499   \n284793  172775.0   1.971002  -0.699067 -1.697541 -0.617643  1.718797   \n284794  172777.0  -1.266580  -0.400461  0.956221 -0.723919  1.531993   \n284795  172778.0 -12.516732  10.187818 -8.476671 -2.510473 -4.586669   \n284796  172780.0   1.884849  -0.143540 -0.999943  1.506772 -0.035300   \n284797  172782.0  -0.241923   0.712247  0.399806 -0.463406  0.244531   \n284798  172782.0   0.219529   0.881246 -0.635891  0.960928 -0.152971   \n284799  172783.0  -1.775135  -0.004235  1.189786  0.331096  1.196063   \n284800  172784.0   2.039560  -0.175233 -1.196825  0.234580 -0.008713   \n284801  172785.0   0.120316   0.931005 -0.546012 -0.745097  1.130314   \n284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n\n              V6        V7        V8        V9  ...         V21       V22  \\\n0       0.462388  0.239599  0.098698  0.363787  ...   -0.018307  0.277838   \n1      -0.082361 -0.078803  0.085102 -0.255425  ...   -0.225775 -0.638672   \n2       1.800499  0.791461  0.247676 -1.514654  ...    0.247998  0.771679   \n3       1.247203  0.237609  0.377436 -1.387024  ...   -0.108300  0.005274   \n4       0.095921  0.592941 -0.270533  0.817739  ...   -0.009431  0.798278   \n5      -0.029728  0.476201  0.260314 -0.568671  ...   -0.208254 -0.559825   \n6       0.272708 -0.005159  0.081213  0.464960  ...   -0.167716 -0.270710   \n7       0.428118  1.120631 -3.807864  0.615375  ...    1.943465 -1.015455   \n8       3.721818  0.370145  0.851084 -0.392048  ...   -0.073425 -0.268092   \n9      -0.246761  0.651583  0.069539 -0.736727  ...   -0.246914 -0.633753   \n10     -0.629152 -1.423236  0.048456 -1.720408  ...   -0.009302  0.313894   \n11      3.317027  0.470455  0.538247 -0.558895  ...    0.049924  0.238422   \n12     -0.753230 -0.689405 -0.227487 -2.094011  ...   -0.231809 -0.483285   \n13      0.337544 -0.096717  0.115982 -0.221083  ...   -0.036876  0.074412   \n14      0.807596 -0.422911 -1.907107  0.755713  ...    1.151663  0.222182   \n15     -0.077850 -0.608581  0.003603 -0.436167  ...    0.499625  1.353650   \n16      0.288069 -0.586057  0.189380  0.782333  ...   -0.024612  0.196002   \n17     -0.127867  0.707642  0.087962 -0.665271  ...   -0.194796 -0.672638   \n18     -1.763406 -1.559738  0.160842  1.233090  ...   -0.503600  0.984460   \n19     -0.720961 -1.080664 -0.053127 -1.978682  ...   -0.177650 -0.175074   \n20      1.309109 -0.878586  0.445290 -0.446196  ...   -0.295583 -0.571955   \n21      1.696038  0.107712  0.521502 -1.191311  ...    0.143997  0.402492   \n22      0.089474  0.241147  0.138082 -0.989162  ...    0.018702 -0.061972   \n23     -0.150116 -0.946365 -1.617935  1.544071  ...    1.650180  0.200454   \n24      2.955053 -0.063063  0.855546  0.049967  ...   -0.579526 -0.799229   \n25     -0.959537  0.543985 -0.104627  0.475664  ...   -0.403639 -0.227404   \n26     -0.916054  0.369025 -0.327260 -0.246651  ...    0.067003  0.227812   \n27     -0.831083 -0.264905 -0.220982 -1.071425  ...   -0.284376 -0.323357   \n28     -0.200331  0.740228 -0.029247 -0.593392  ...    0.077237  0.457331   \n29      0.578435 -0.767084  0.401046  0.699500  ...    0.013676  0.213734   \n...          ...       ...       ...       ...  ...         ...       ...   \n284777 -1.345452  0.227476 -0.378355  0.665911  ...    0.235758  0.829758   \n284778 -0.760802  0.758545  0.414698 -0.730854  ...    0.003530 -0.431876   \n284779  3.664740 -0.533297  0.842937  1.128798  ...    0.086043  0.543613   \n284780  0.793083 -0.527298  0.866429  0.853819  ...   -0.094708  0.236818   \n284781  0.093598  0.191353  0.092211 -0.062621  ...   -0.191027 -0.631658   \n284782  0.186479 -0.045911  0.936448 -2.419986  ...   -0.263889 -0.857904   \n284783  3.123732 -0.270714  1.657495  0.465804  ...    0.271170  1.145750   \n284784  3.240843  0.181576  1.282746 -0.893890  ...    0.183856  0.202670   \n284785  3.401529  0.337434  0.925377 -0.165663  ...   -0.266113 -0.716336   \n284786  3.732950 -1.217430 -0.536644  0.272867  ...    2.016666 -1.588269   \n284787 -2.956733  0.283610 -0.332656 -0.247488  ...    0.353722  0.488487   \n284788 -0.751373 -0.458972 -0.140140  0.959971  ...   -0.208260 -0.430347   \n284789 -0.605641  1.253430 -1.042610 -0.417116  ...    0.851800  0.305268   \n284790 -0.316187  0.396137  0.532364 -0.224606  ...   -0.280302 -0.849919   \n284791 -1.046238  0.757051  0.230473 -0.506856  ...   -0.108846 -0.480820   \n284792 -0.482638  0.548393  0.343003 -0.226323  ...    0.414621  1.307511   \n284793  3.911336 -1.259306  1.056209  1.315006  ...    0.188758  0.694418   \n284794 -1.788600  0.314741  0.004704  0.013857  ...   -0.157831 -0.883365   \n284795 -1.394465 -3.632516  5.498583  4.893089  ...   -0.944759 -1.565026   \n284796 -0.613638  0.190241 -0.249058  0.666458  ...    0.144008  0.634646   \n284797 -1.343668  0.929369 -0.206210  0.106234  ...   -0.228876 -0.514376   \n284798 -1.014307  0.427126  0.121340 -0.285670  ...    0.099936  0.337120   \n284799  5.519980 -1.518185  2.080825  1.159498  ...    0.103302  0.654850   \n284800 -0.726571  0.017050 -0.118228  0.435402  ...   -0.268048 -0.717211   \n284801 -0.235973  0.812722  0.115093 -0.204064  ...   -0.314205 -0.808520   \n284802 -2.606837 -4.918215  7.305334  1.914428  ...    0.213454  0.111864   \n284803  1.058415  0.024330  0.294869  0.584800  ...    0.214205  0.924384   \n284804  3.031260 -0.296827  0.708417  0.432454  ...    0.232045  0.578229   \n284805  0.623708 -0.686180  0.679145  0.392087  ...    0.265245  0.800049   \n284806 -0.649617  1.577006 -0.414650  0.486180  ...    0.261057  0.643078   \n\n             V23       V24       V25       V26       V27       V28  Amount  \\\n0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n5      -0.026398 -0.371427 -0.232794  0.105915  0.253844  0.081080    3.67   \n6      -0.154104 -0.780055  0.750137 -0.257237  0.034507  0.005168    4.99   \n7       0.057504 -0.649709 -0.415267 -0.051634 -1.206921 -1.085339   40.80   \n8      -0.204233  1.011592  0.373205 -0.384157  0.011747  0.142404   93.20   \n9      -0.120794 -0.385050 -0.069733  0.094199  0.246219  0.083076    3.68   \n10      0.027740  0.500512  0.251367 -0.129478  0.042850  0.016253    7.80   \n11      0.009130  0.996710 -0.767315 -0.492208  0.042472 -0.054337    9.99   \n12      0.084668  0.392831  0.161135 -0.354990  0.026416  0.042422  121.50   \n13     -0.071407  0.104744  0.548265  0.104094  0.021491  0.021293   27.50   \n14      1.020586  0.028317 -0.232746 -0.235557 -0.164778 -0.030154   58.80   \n15     -0.256573 -0.065084 -0.039124 -0.087086 -0.180998  0.129394   15.99   \n16      0.013802  0.103758  0.364298 -0.382261  0.092809  0.037051   12.99   \n17     -0.156858 -0.888386 -0.342413 -0.049027  0.079692  0.131024    0.89   \n18      2.458589  0.042119 -0.481631 -0.621272  0.392053  0.949594   46.80   \n19      0.040002  0.295814  0.332931 -0.220385  0.022298  0.007602    5.00   \n20     -0.050881 -0.304215  0.072001 -0.422234  0.086553  0.063499  231.71   \n21     -0.048508 -1.371866  0.390814  0.199964  0.016371 -0.014605   34.09   \n22     -0.103855 -0.370415  0.603200  0.108556 -0.040521 -0.011418    2.28   \n23     -0.185353  0.423073  0.820591 -0.227632  0.336634  0.250475   22.75   \n24      0.870300  0.983421  0.321201  0.149650  0.707519  0.014600    0.89   \n25      0.742435  0.398535  0.249212  0.274404  0.359969  0.243232   26.43   \n26     -0.150487  0.435045  0.724825 -0.337082  0.016368  0.030041   41.88   \n27     -0.037710  0.347151  0.559639 -0.280158  0.042335  0.028822   16.00   \n28     -0.038500  0.642522 -0.183891 -0.277464  0.182687  0.152665   33.00   \n29      0.014462  0.002951  0.294638 -0.395070  0.081461  0.024220   12.99   \n...          ...       ...       ...       ...       ...       ...     ...   \n284777 -0.002063  0.001344  0.262183 -0.105327 -0.022363 -0.060283    1.00   \n284778  0.141759  0.587119 -0.200998  0.267337 -0.152951 -0.065285   80.00   \n284779 -0.032129  0.768379  0.477688 -0.031833  0.014151 -0.066542   25.00   \n284780 -0.204280  1.158185  0.627801 -0.399981  0.510818  0.233265   30.00   \n284781 -0.147249  0.212931  0.354257 -0.241068 -0.161717 -0.149188   13.00   \n284782  0.235172 -0.681794 -0.668894  0.044657 -0.066751 -0.072447   12.82   \n284783  0.084783  0.721269 -0.529906 -0.240117  0.129126 -0.080620   11.46   \n284784 -0.373023  0.651122  1.073823  0.844590 -0.286676 -0.187719   40.00   \n284785  0.108519  0.688519 -0.460220  0.161939  0.265368  0.090245    1.79   \n284786  0.588482  0.632444 -0.201064  0.199251  0.438657  0.172923    8.95   \n284787  0.293632  0.107812 -0.935586  1.138216  0.025271  0.255347    9.99   \n284788  0.416765  0.064819 -0.608337  0.268436 -0.028069 -0.041367    3.99   \n284789 -0.148093 -0.038712  0.010209 -0.362666  0.503092  0.229921   60.50   \n284790  0.300245  0.000607 -0.376379  0.128660 -0.015205 -0.021486    9.81   \n284791 -0.074513 -0.003988 -0.113149  0.280378 -0.077310  0.023079   20.32   \n284792 -0.059545  0.242669 -0.665424 -0.269869 -0.170579 -0.030692    3.99   \n284793  0.163002  0.726365 -0.058282 -0.191813  0.061858 -0.043716    4.99   \n284794  0.088485 -0.076790 -0.095833  0.132720 -0.028468  0.126494    0.89   \n284795  0.890675 -1.253276  1.786717  0.320763  2.090712  1.232864    9.87   \n284796 -0.042114 -0.053206  0.316403 -0.461441  0.018265 -0.041068   60.00   \n284797  0.279598  0.371441 -0.559238  0.113144  0.131507  0.081265    5.49   \n284798  0.251791  0.057688 -1.508368  0.144023  0.181205  0.215243   24.05   \n284799 -0.348929  0.745323  0.704545 -0.127579  0.454379  0.130308   79.99   \n284800  0.297930 -0.359769 -0.315610  0.201114 -0.080826 -0.075071    2.68   \n284801  0.050343  0.102800 -0.435870  0.124079  0.217940  0.068803    2.69   \n284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n\n        Class  \n0           0  \n1           0  \n2           0  \n3           0  \n4           0  \n5           0  \n6           0  \n7           0  \n8           0  \n9           0  \n10          0  \n11          0  \n12          0  \n13          0  \n14          0  \n15          0  \n16          0  \n17          0  \n18          0  \n19          0  \n20          0  \n21          0  \n22          0  \n23          0  \n24          0  \n25          0  \n26          0  \n27          0  \n28          0  \n29          0  \n...       ...  \n284777      0  \n284778      0  \n284779      0  \n284780      0  \n284781      0  \n284782      0  \n284783      0  \n284784      0  \n284785      0  \n284786      0  \n284787      0  \n284788      0  \n284789      0  \n284790      0  \n284791      0  \n284792      0  \n284793      0  \n284794      0  \n284795      0  \n284796      0  \n284797      0  \n284798      0  \n284799      0  \n284800      0  \n284801      0  \n284802      0  \n284803      0  \n284804      0  \n284805      0  \n284806      0  \n\n[284807 rows x 31 columns] is not an estimator instance.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-91c2357751cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m dot_data = export_graphviz(data, out_file=None, feature_names=['class1', 'class0'],\n\u001b[1;32m----> 6\u001b[1;33m                           class_names=data['Class'], filled=True, rounded=True, special_characters=True)\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydotplus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_from_dot_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdot_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_png\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\export.py\u001b[0m in \u001b[0;36mexport_graphviz\u001b[1;34m(decision_tree, out_file, max_depth, feature_names, class_names, label, filled, leaves_parallel, impurity, node_ids, proportion, rotate, rounded, special_characters, precision)\u001b[0m\n\u001b[0;32m    390\u001b[0m                 \u001b[0mout_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%d -> %d ;\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m     \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecision_tree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tree_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m     \u001b[0mown_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m     \u001b[0mreturn_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m    760\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not an estimator instance.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattributes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m:             Time         V1         V2        V3        V4        V5  \\\n0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n5            2.0  -0.425966   0.960523  1.141109 -0.168252  0.420987   \n6            4.0   1.229658   0.141004  0.045371  1.202613  0.191881   \n7            7.0  -0.644269   1.417964  1.074380 -0.492199  0.948934   \n8            7.0  -0.894286   0.286157 -0.113192 -0.271526  2.669599   \n9            9.0  -0.338262   1.119593  1.044367 -0.222187  0.499361   \n10          10.0   1.449044  -1.176339  0.913860 -1.375667 -1.971383   \n11          10.0   0.384978   0.616109 -0.874300 -0.094019  2.924584   \n12          10.0   1.249999  -1.221637  0.383930 -1.234899 -1.485419   \n13          11.0   1.069374   0.287722  0.828613  2.712520 -0.178398   \n14          12.0  -2.791855  -0.327771  1.641750  1.767473 -0.136588   \n15          12.0  -0.752417   0.345485  2.057323 -1.468643 -1.158394   \n16          12.0   1.103215  -0.040296  1.267332  1.289091 -0.735997   \n17          13.0  -0.436905   0.918966  0.924591 -0.727219  0.915679   \n18          14.0  -5.401258  -5.450148  1.186305  1.736239  3.049106   \n19          15.0   1.492936  -1.029346  0.454795 -1.438026 -1.555434   \n20          16.0   0.694885  -1.361819  1.029221  0.834159 -1.191209   \n21          17.0   0.962496   0.328461 -0.171479  2.109204  1.129566   \n22          18.0   1.166616   0.502120 -0.067300  2.261569  0.428804   \n23          18.0   0.247491   0.277666  1.185471 -0.092603 -1.314394   \n24          22.0  -1.946525  -0.044901 -0.405570 -1.013057  2.941968   \n25          22.0  -2.074295  -0.121482  1.322021  0.410008  0.295198   \n26          23.0   1.173285   0.353498  0.283905  1.133563 -0.172577   \n27          23.0   1.322707  -0.174041  0.434555  0.576038 -0.836758   \n28          23.0  -0.414289   0.905437  1.727453  1.473471  0.007443   \n29          23.0   1.059387  -0.175319  1.266130  1.186110 -0.786002   \n...          ...        ...        ...       ...       ...       ...   \n284777  172764.0   2.079137  -0.028723 -1.343392  0.358000 -0.045791   \n284778  172764.0  -0.764523   0.588379 -0.907599 -0.418847  0.901528   \n284779  172766.0   1.975178  -0.616244 -2.628295 -0.406246  2.327804   \n284780  172766.0  -1.727503   1.108356  2.219561  1.148583 -0.884199   \n284781  172766.0  -1.139015  -0.155510  1.894478 -1.138957  1.451777   \n284782  172767.0  -0.268061   2.540315 -1.400915  4.846661  0.639105   \n284783  172768.0  -1.796092   1.929178 -2.828417 -1.689844  2.199572   \n284784  172768.0  -0.669662   0.923769 -1.543167 -1.560729  2.833960   \n284785  172768.0   0.032887   0.545338 -1.185844 -1.729828  2.932315   \n284786  172768.0  -2.076175   2.142238 -2.522704 -1.888063  1.982785   \n284787  172769.0  -1.029719  -1.110670 -0.636179 -0.840816  2.424360   \n284788  172770.0   2.007418  -0.280235 -0.208113  0.335261 -0.715798   \n284789  172770.0  -0.446951   1.302212 -0.168583  0.981577  0.578957   \n284790  172771.0  -0.515513   0.971950 -1.014580 -0.677037  0.912430   \n284791  172774.0  -0.863506   0.874701  0.420358 -0.530365  0.356561   \n284792  172774.0  -0.724123   1.485216 -1.132218 -0.607190  0.709499   \n284793  172775.0   1.971002  -0.699067 -1.697541 -0.617643  1.718797   \n284794  172777.0  -1.266580  -0.400461  0.956221 -0.723919  1.531993   \n284795  172778.0 -12.516732  10.187818 -8.476671 -2.510473 -4.586669   \n284796  172780.0   1.884849  -0.143540 -0.999943  1.506772 -0.035300   \n284797  172782.0  -0.241923   0.712247  0.399806 -0.463406  0.244531   \n284798  172782.0   0.219529   0.881246 -0.635891  0.960928 -0.152971   \n284799  172783.0  -1.775135  -0.004235  1.189786  0.331096  1.196063   \n284800  172784.0   2.039560  -0.175233 -1.196825  0.234580 -0.008713   \n284801  172785.0   0.120316   0.931005 -0.546012 -0.745097  1.130314   \n284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n\n              V6        V7        V8        V9  ...         V21       V22  \\\n0       0.462388  0.239599  0.098698  0.363787  ...   -0.018307  0.277838   \n1      -0.082361 -0.078803  0.085102 -0.255425  ...   -0.225775 -0.638672   \n2       1.800499  0.791461  0.247676 -1.514654  ...    0.247998  0.771679   \n3       1.247203  0.237609  0.377436 -1.387024  ...   -0.108300  0.005274   \n4       0.095921  0.592941 -0.270533  0.817739  ...   -0.009431  0.798278   \n5      -0.029728  0.476201  0.260314 -0.568671  ...   -0.208254 -0.559825   \n6       0.272708 -0.005159  0.081213  0.464960  ...   -0.167716 -0.270710   \n7       0.428118  1.120631 -3.807864  0.615375  ...    1.943465 -1.015455   \n8       3.721818  0.370145  0.851084 -0.392048  ...   -0.073425 -0.268092   \n9      -0.246761  0.651583  0.069539 -0.736727  ...   -0.246914 -0.633753   \n10     -0.629152 -1.423236  0.048456 -1.720408  ...   -0.009302  0.313894   \n11      3.317027  0.470455  0.538247 -0.558895  ...    0.049924  0.238422   \n12     -0.753230 -0.689405 -0.227487 -2.094011  ...   -0.231809 -0.483285   \n13      0.337544 -0.096717  0.115982 -0.221083  ...   -0.036876  0.074412   \n14      0.807596 -0.422911 -1.907107  0.755713  ...    1.151663  0.222182   \n15     -0.077850 -0.608581  0.003603 -0.436167  ...    0.499625  1.353650   \n16      0.288069 -0.586057  0.189380  0.782333  ...   -0.024612  0.196002   \n17     -0.127867  0.707642  0.087962 -0.665271  ...   -0.194796 -0.672638   \n18     -1.763406 -1.559738  0.160842  1.233090  ...   -0.503600  0.984460   \n19     -0.720961 -1.080664 -0.053127 -1.978682  ...   -0.177650 -0.175074   \n20      1.309109 -0.878586  0.445290 -0.446196  ...   -0.295583 -0.571955   \n21      1.696038  0.107712  0.521502 -1.191311  ...    0.143997  0.402492   \n22      0.089474  0.241147  0.138082 -0.989162  ...    0.018702 -0.061972   \n23     -0.150116 -0.946365 -1.617935  1.544071  ...    1.650180  0.200454   \n24      2.955053 -0.063063  0.855546  0.049967  ...   -0.579526 -0.799229   \n25     -0.959537  0.543985 -0.104627  0.475664  ...   -0.403639 -0.227404   \n26     -0.916054  0.369025 -0.327260 -0.246651  ...    0.067003  0.227812   \n27     -0.831083 -0.264905 -0.220982 -1.071425  ...   -0.284376 -0.323357   \n28     -0.200331  0.740228 -0.029247 -0.593392  ...    0.077237  0.457331   \n29      0.578435 -0.767084  0.401046  0.699500  ...    0.013676  0.213734   \n...          ...       ...       ...       ...  ...         ...       ...   \n284777 -1.345452  0.227476 -0.378355  0.665911  ...    0.235758  0.829758   \n284778 -0.760802  0.758545  0.414698 -0.730854  ...    0.003530 -0.431876   \n284779  3.664740 -0.533297  0.842937  1.128798  ...    0.086043  0.543613   \n284780  0.793083 -0.527298  0.866429  0.853819  ...   -0.094708  0.236818   \n284781  0.093598  0.191353  0.092211 -0.062621  ...   -0.191027 -0.631658   \n284782  0.186479 -0.045911  0.936448 -2.419986  ...   -0.263889 -0.857904   \n284783  3.123732 -0.270714  1.657495  0.465804  ...    0.271170  1.145750   \n284784  3.240843  0.181576  1.282746 -0.893890  ...    0.183856  0.202670   \n284785  3.401529  0.337434  0.925377 -0.165663  ...   -0.266113 -0.716336   \n284786  3.732950 -1.217430 -0.536644  0.272867  ...    2.016666 -1.588269   \n284787 -2.956733  0.283610 -0.332656 -0.247488  ...    0.353722  0.488487   \n284788 -0.751373 -0.458972 -0.140140  0.959971  ...   -0.208260 -0.430347   \n284789 -0.605641  1.253430 -1.042610 -0.417116  ...    0.851800  0.305268   \n284790 -0.316187  0.396137  0.532364 -0.224606  ...   -0.280302 -0.849919   \n284791 -1.046238  0.757051  0.230473 -0.506856  ...   -0.108846 -0.480820   \n284792 -0.482638  0.548393  0.343003 -0.226323  ...    0.414621  1.307511   \n284793  3.911336 -1.259306  1.056209  1.315006  ...    0.188758  0.694418   \n284794 -1.788600  0.314741  0.004704  0.013857  ...   -0.157831 -0.883365   \n284795 -1.394465 -3.632516  5.498583  4.893089  ...   -0.944759 -1.565026   \n284796 -0.613638  0.190241 -0.249058  0.666458  ...    0.144008  0.634646   \n284797 -1.343668  0.929369 -0.206210  0.106234  ...   -0.228876 -0.514376   \n284798 -1.014307  0.427126  0.121340 -0.285670  ...    0.099936  0.337120   \n284799  5.519980 -1.518185  2.080825  1.159498  ...    0.103302  0.654850   \n284800 -0.726571  0.017050 -0.118228  0.435402  ...   -0.268048 -0.717211   \n284801 -0.235973  0.812722  0.115093 -0.204064  ...   -0.314205 -0.808520   \n284802 -2.606837 -4.918215  7.305334  1.914428  ...    0.213454  0.111864   \n284803  1.058415  0.024330  0.294869  0.584800  ...    0.214205  0.924384   \n284804  3.031260 -0.296827  0.708417  0.432454  ...    0.232045  0.578229   \n284805  0.623708 -0.686180  0.679145  0.392087  ...    0.265245  0.800049   \n284806 -0.649617  1.577006 -0.414650  0.486180  ...    0.261057  0.643078   \n\n             V23       V24       V25       V26       V27       V28  Amount  \\\n0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n5      -0.026398 -0.371427 -0.232794  0.105915  0.253844  0.081080    3.67   \n6      -0.154104 -0.780055  0.750137 -0.257237  0.034507  0.005168    4.99   \n7       0.057504 -0.649709 -0.415267 -0.051634 -1.206921 -1.085339   40.80   \n8      -0.204233  1.011592  0.373205 -0.384157  0.011747  0.142404   93.20   \n9      -0.120794 -0.385050 -0.069733  0.094199  0.246219  0.083076    3.68   \n10      0.027740  0.500512  0.251367 -0.129478  0.042850  0.016253    7.80   \n11      0.009130  0.996710 -0.767315 -0.492208  0.042472 -0.054337    9.99   \n12      0.084668  0.392831  0.161135 -0.354990  0.026416  0.042422  121.50   \n13     -0.071407  0.104744  0.548265  0.104094  0.021491  0.021293   27.50   \n14      1.020586  0.028317 -0.232746 -0.235557 -0.164778 -0.030154   58.80   \n15     -0.256573 -0.065084 -0.039124 -0.087086 -0.180998  0.129394   15.99   \n16      0.013802  0.103758  0.364298 -0.382261  0.092809  0.037051   12.99   \n17     -0.156858 -0.888386 -0.342413 -0.049027  0.079692  0.131024    0.89   \n18      2.458589  0.042119 -0.481631 -0.621272  0.392053  0.949594   46.80   \n19      0.040002  0.295814  0.332931 -0.220385  0.022298  0.007602    5.00   \n20     -0.050881 -0.304215  0.072001 -0.422234  0.086553  0.063499  231.71   \n21     -0.048508 -1.371866  0.390814  0.199964  0.016371 -0.014605   34.09   \n22     -0.103855 -0.370415  0.603200  0.108556 -0.040521 -0.011418    2.28   \n23     -0.185353  0.423073  0.820591 -0.227632  0.336634  0.250475   22.75   \n24      0.870300  0.983421  0.321201  0.149650  0.707519  0.014600    0.89   \n25      0.742435  0.398535  0.249212  0.274404  0.359969  0.243232   26.43   \n26     -0.150487  0.435045  0.724825 -0.337082  0.016368  0.030041   41.88   \n27     -0.037710  0.347151  0.559639 -0.280158  0.042335  0.028822   16.00   \n28     -0.038500  0.642522 -0.183891 -0.277464  0.182687  0.152665   33.00   \n29      0.014462  0.002951  0.294638 -0.395070  0.081461  0.024220   12.99   \n...          ...       ...       ...       ...       ...       ...     ...   \n284777 -0.002063  0.001344  0.262183 -0.105327 -0.022363 -0.060283    1.00   \n284778  0.141759  0.587119 -0.200998  0.267337 -0.152951 -0.065285   80.00   \n284779 -0.032129  0.768379  0.477688 -0.031833  0.014151 -0.066542   25.00   \n284780 -0.204280  1.158185  0.627801 -0.399981  0.510818  0.233265   30.00   \n284781 -0.147249  0.212931  0.354257 -0.241068 -0.161717 -0.149188   13.00   \n284782  0.235172 -0.681794 -0.668894  0.044657 -0.066751 -0.072447   12.82   \n284783  0.084783  0.721269 -0.529906 -0.240117  0.129126 -0.080620   11.46   \n284784 -0.373023  0.651122  1.073823  0.844590 -0.286676 -0.187719   40.00   \n284785  0.108519  0.688519 -0.460220  0.161939  0.265368  0.090245    1.79   \n284786  0.588482  0.632444 -0.201064  0.199251  0.438657  0.172923    8.95   \n284787  0.293632  0.107812 -0.935586  1.138216  0.025271  0.255347    9.99   \n284788  0.416765  0.064819 -0.608337  0.268436 -0.028069 -0.041367    3.99   \n284789 -0.148093 -0.038712  0.010209 -0.362666  0.503092  0.229921   60.50   \n284790  0.300245  0.000607 -0.376379  0.128660 -0.015205 -0.021486    9.81   \n284791 -0.074513 -0.003988 -0.113149  0.280378 -0.077310  0.023079   20.32   \n284792 -0.059545  0.242669 -0.665424 -0.269869 -0.170579 -0.030692    3.99   \n284793  0.163002  0.726365 -0.058282 -0.191813  0.061858 -0.043716    4.99   \n284794  0.088485 -0.076790 -0.095833  0.132720 -0.028468  0.126494    0.89   \n284795  0.890675 -1.253276  1.786717  0.320763  2.090712  1.232864    9.87   \n284796 -0.042114 -0.053206  0.316403 -0.461441  0.018265 -0.041068   60.00   \n284797  0.279598  0.371441 -0.559238  0.113144  0.131507  0.081265    5.49   \n284798  0.251791  0.057688 -1.508368  0.144023  0.181205  0.215243   24.05   \n284799 -0.348929  0.745323  0.704545 -0.127579  0.454379  0.130308   79.99   \n284800  0.297930 -0.359769 -0.315610  0.201114 -0.080826 -0.075071    2.68   \n284801  0.050343  0.102800 -0.435870  0.124079  0.217940  0.068803    2.69   \n284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n\n        Class  \n0           0  \n1           0  \n2           0  \n3           0  \n4           0  \n5           0  \n6           0  \n7           0  \n8           0  \n9           0  \n10          0  \n11          0  \n12          0  \n13          0  \n14          0  \n15          0  \n16          0  \n17          0  \n18          0  \n19          0  \n20          0  \n21          0  \n22          0  \n23          0  \n24          0  \n25          0  \n26          0  \n27          0  \n28          0  \n29          0  \n...       ...  \n284777      0  \n284778      0  \n284779      0  \n284780      0  \n284781      0  \n284782      0  \n284783      0  \n284784      0  \n284785      0  \n284786      0  \n284787      0  \n284788      0  \n284789      0  \n284790      0  \n284791      0  \n284792      0  \n284793      0  \n284794      0  \n284795      0  \n284796      0  \n284797      0  \n284798      0  \n284799      0  \n284800      0  \n284801      0  \n284802      0  \n284803      0  \n284804      0  \n284805      0  \n284806      0  \n\n[284807 rows x 31 columns] is not an estimator instance."
     ]
    }
   ],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "from IPython.display import Image\n",
    "\n",
    "dot_data = export_graphviz(data, out_file=None, feature_names=['class1', 'class0'],\n",
    "                          class_names=data['Class'], filled=True, rounded=True, special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.로짓알고리즘 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "logit = linear_model.LogisticRegression()\n",
    "logit.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.13383633])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.18396475,  0.51464992, -0.461006  ,  0.11429066, -0.79091681,\n",
       "        -0.14646869]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_l = logit.predict(X_test)\n",
    "pred_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56873"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test[y_test==0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모형검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Class</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56864</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Class      0   1\n",
       "row_0           \n",
       "0      56864  40\n",
       "1          9  49"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(pred_l, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Type 0</th>\n",
       "      <th>Predicted  Type 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Type 0</th>\n",
       "      <td>56864</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Type 1</th>\n",
       "      <td>40</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Predicted Type 0  Predicted  Type 1\n",
       "True Type 0             56864                  9\n",
       "True Type 1                40                 49"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matrix = pd.DataFrame(\n",
    "    confusion_matrix(y_test, pred_l),\n",
    "    columns=['Predicted Type 0', 'Predicted  Type 1'],\n",
    "    index=['True Type 0', 'True Type 1']\n",
    ")\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy:  0.9991397773954567\n"
     ]
    }
   ],
   "source": [
    "print('Prediction Accuracy: ', logit.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree와 Logit 정확성비교\n",
    "(Tree가 아주 근소한 차이로 우수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.022225343200628e-05"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X = X_test, y = y_test)-logit.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
